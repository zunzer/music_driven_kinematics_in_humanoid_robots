{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "287418a1",
   "metadata": {},
   "source": [
    "# Genre Identifier\n",
    "\n",
    "Creates a neural network that recognizes the genre of a song\n",
    "\n",
    "feature explanation courtesy of:\n",
    "https://navdeepsinghh.medium.com/identifying-the-genre-of-a-song-with-neural-networks-851db89c42f0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baf5224f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/cornelius/anaconda3/envs/robocup/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/cornelius/anaconda3/envs/robocup/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/cornelius/anaconda3/envs/robocup/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/cornelius/anaconda3/envs/robocup/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/cornelius/anaconda3/envs/robocup/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/cornelius/anaconda3/envs/robocup/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/cornelius/anaconda3/envs/robocup/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/cornelius/anaconda3/envs/robocup/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/cornelius/anaconda3/envs/robocup/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/cornelius/anaconda3/envs/robocup/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/cornelius/anaconda3/envs/robocup/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/cornelius/anaconda3/envs/robocup/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import librosa.feature\n",
    "import librosa.display\n",
    "import glob\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "from scipy.signal import butter, lfilter\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout,  Conv1D, Conv2D, Flatten, BatchNormalization, ZeroPadding2D,  MaxPooling2D, GlobalMaxPooling2D, GlobalAveragePooling1D, AveragePooling2D, Input, Add\n",
    "from keras.utils.np_utils import to_categorical\n",
    "# from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc09bce6",
   "metadata": {},
   "source": [
    "## 1. Load the data\n",
    "\n",
    "Load the data into one vector containg all the information. Data we use for training and testing is the GTZAN data set (https://www.tensorflow.org/datasets/catalog/gtzan)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c65822c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_f(file):\n",
    "    lo, hi = 600,10000\n",
    "    y, sr = librosa.load(file)\n",
    "    b, a = butter(N=6, Wn=[2*lo/sr, 2*hi/sr], btype='band')\n",
    "    x = lfilter(b,a,y)\n",
    "    return x, sr\n",
    "\n",
    "def compress_filter(file):\n",
    "    y, sr = librosa.load(file)\n",
    "    x = librosa.mu_compress(y, quantize=False)\n",
    "    return x, sr\n",
    "\n",
    "def extract_song_features(f):\n",
    "    #y, _ = librosa.load(f)\n",
    "    y, sr = compress_filter(f)\n",
    "\n",
    "    # get Mel-frequency cepstral coefficients and normalize\n",
    "    mfcc = librosa.feature.mfcc(y)\n",
    "    mfcc /= np.amax(np.absolute(mfcc))\n",
    "    return np.ndarray.flatten(mfcc)[:25000]\n",
    "    \n",
    "    # get melspectrogram\n",
    "    #spect = librosa.feature.melspectrogram(y=y, sr=sr,n_fft=2048, hop_length=512)\n",
    "    #spect = librosa.power_to_db(spect, ref=np.max)\n",
    "    #spect /= np.amax(np.absolute(spect))\n",
    "    #return np.ndarray.flatten(spect)[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d9f42c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 100 songs in classical genre...\n",
      "Processing 100 songs in jazz genre...\n",
      "Processing 100 songs in metal genre...\n"
     ]
    }
   ],
   "source": [
    "# GENRES = ['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']\n",
    "GENRES = ['classical', 'jazz', 'metal']\n",
    "all_features = []\n",
    "all_labels = []\n",
    "\n",
    "# load all songs from the gtzan data set\n",
    "for genre in GENRES:\n",
    "    sound_files = glob.glob('genres/' + genre + '/*.wav')\n",
    "    print('Processing %d songs in %s genre...' % (len(sound_files), genre))\n",
    "    for f in sound_files:\n",
    "        extracted_features = extract_song_features(f)\n",
    "        all_features.append(extracted_features)\n",
    "        all_labels.append(genre)\n",
    "\n",
    "# convert labels to one-hot encoding\n",
    "label_uniq_ids, label_row_ids = np.unique(all_labels, return_inverse=True)\n",
    "label_row_ids = label_row_ids.astype(np.int32, copy=False)\n",
    "onehot_labels = to_categorical(label_row_ids, len(label_uniq_ids))\n",
    "\n",
    "# store features and labels\n",
    "features = np.stack(all_features)\n",
    "labels = onehot_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d7309a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 25000)\n",
      "(300, 3)\n"
     ]
    }
   ],
   "source": [
    "# get shapes\n",
    "print(np.shape(features))\n",
    "print(np.shape(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56bf6ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "208f478e",
   "metadata": {},
   "source": [
    "# Perceptron\n",
    "## 2. Prepare the data set and build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53856691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(data, training_split=0.8, n_classes=10):\n",
    "    \"\"\"\n",
    "    this splits according to the ration we want to split with\n",
    "    \"\"\"\n",
    "    np.random.shuffle(data)\n",
    "    split_idx = int(len(data) * training_split)\n",
    "    train, test = data[:split_idx, :], data[split_idx:, :]\n",
    "\n",
    "    #      train data,     train labels,  test data,      test labels\n",
    "    return train[:, :-n_classes], train[:, -n_classes:], test[:, :-n_classes], test[:, -n_classes:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b890b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(train):\n",
    "    \"\"\"\n",
    "    build the model; in our case it is a perceptron with 100 layers\n",
    "    \"\"\"\n",
    "    input_shape = np.shape(train[0])\n",
    "    print(f\"shape of input data: {input_shape}\")\n",
    "    nn_model = Sequential([\n",
    "        Dense(100, input_dim=np.shape(train)[1]),\n",
    "        Activation('relu'),\n",
    "        Dense(10),\n",
    "        Activation('softmax'),\n",
    "    ])\n",
    "\n",
    "    # specify which techniques you want to use for training\n",
    "    nn_model.compile(optimizer='adam',\n",
    "                     loss='categorical_crossentropy',\n",
    "                     metrics=['accuracy'])\n",
    "    print(nn_model.summary())\n",
    "    return nn_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1735c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of input data: (24993,)\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 100)               2499400   \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                1010      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 2,500,410\n",
      "Trainable params: 2,500,410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "shape of input data: (240, 24993)\n"
     ]
    }
   ],
   "source": [
    "# last column has genre, turn it into unique ids\n",
    "all_data = np.column_stack((features, labels))\n",
    "\n",
    "# split into training and test data\n",
    "train_input, train_labels, test_input, test_labels = split_train_test(all_data)\n",
    "\n",
    "# Reshape for CNN input\n",
    "#train_in = np.array([x.reshape( (125, 200, 1) ) for x in train_input])\n",
    "#test_in = np.array([x.reshape( (125, 200, 1) ) for x in test_input])\n",
    "\n",
    "# build the nn model\n",
    "model = build_model(train_input)\n",
    "print(f\"shape of input data: {np.shape(train_input)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff16acac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 25003)\n",
      "(240, 24993)\n",
      "(240, 10)\n"
     ]
    }
   ],
   "source": [
    "print(all_data.shape)\n",
    "print(train_input.shape)\n",
    "print(train_labels.shape)\n",
    "#print(train_in.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5105c82b",
   "metadata": {},
   "source": [
    "## 3. Run the process\n",
    "\n",
    "Now, we train our model and evaluate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f5c40b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/cornelius/anaconda3/envs/robocup/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 192 samples, validate on 48 samples\n",
      "Epoch 1/10\n",
      "192/192 [==============================] - 1s 4ms/step - loss: 0.5049 - accuracy: 0.6042 - val_loss: -0.6380 - val_accuracy: 0.8125\n",
      "Epoch 2/10\n",
      "192/192 [==============================] - 0s 2ms/step - loss: -0.6287 - accuracy: 0.9323 - val_loss: -1.5409 - val_accuracy: 0.7500\n",
      "Epoch 3/10\n",
      "192/192 [==============================] - 0s 2ms/step - loss: -1.1319 - accuracy: 0.9583 - val_loss: -2.1212 - val_accuracy: 0.7917\n",
      "Epoch 4/10\n",
      "192/192 [==============================] - 0s 2ms/step - loss: -1.5027 - accuracy: 0.9896 - val_loss: -2.7315 - val_accuracy: 0.8333\n",
      "Epoch 5/10\n",
      "192/192 [==============================] - 0s 2ms/step - loss: -1.9934 - accuracy: 1.0000 - val_loss: -3.3579 - val_accuracy: 0.7917\n",
      "Epoch 6/10\n",
      "192/192 [==============================] - 0s 2ms/step - loss: -2.4730 - accuracy: 1.0000 - val_loss: -4.0652 - val_accuracy: 0.8333\n",
      "Epoch 7/10\n",
      "192/192 [==============================] - 1s 3ms/step - loss: -3.0662 - accuracy: 1.0000 - val_loss: -4.8933 - val_accuracy: 0.8125\n",
      "Epoch 8/10\n",
      "192/192 [==============================] - 0s 2ms/step - loss: -3.7196 - accuracy: 1.0000 - val_loss: -5.9221 - val_accuracy: 0.8125\n",
      "Epoch 9/10\n",
      "192/192 [==============================] - 0s 2ms/step - loss: -4.4920 - accuracy: 1.0000 - val_loss: -7.1927 - val_accuracy: 0.7917\n",
      "Epoch 10/10\n",
      "192/192 [==============================] - 0s 2ms/step - loss: -5.5359 - accuracy: 1.0000 - val_loss: -8.5335 - val_accuracy: 0.8125\n",
      "60/60 [==============================] - 0s 368us/step\n",
      "Done!\n",
      "Loss: -5.5075, accuracy: 0.8500\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "model.fit(train_input, train_labels, epochs=10, batch_size=32,\n",
    "          validation_split=0.2)\n",
    "\n",
    "# now get the performance indicators\n",
    "loss, acc = model.evaluate(test_input, test_labels, batch_size=32)\n",
    "\n",
    "print(\"Done!\")\n",
    "print(\"Loss: %.4f, accuracy: %.4f\" % (loss, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13f5caf",
   "metadata": {},
   "source": [
    "## 4. Store the classifier to use it in thinking.py\n",
    "\n",
    "use JSON for simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76d95f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "model.save(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49df6159",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36daa1b2",
   "metadata": {},
   "source": [
    "## 5. Retrieve model and test it to see if everything works\n",
    "\n",
    "make sure you have the correct version of h5py (2.10.0). This creates warnings at one end, but was the only thing that worked for me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "01208bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 100)               2500100   \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                1010      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 2,501,110\n",
      "Trainable params: 2,501,110\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model = load_model('model.h5')\n",
    "print(\"Loaded model from disk\")\n",
    "# summarize model.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6bd409cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 25000)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y, _ = librosa.load(\"../recordings/output.wav\")\n",
    "# get Mel-frequency cepstral coefficients and normalize\n",
    "mfcc = librosa.feature.mfcc(y)\n",
    "mfcc /= np.amax(np.absolute(mfcc))\n",
    "own_audio = np.ndarray.flatten(mfcc)[:25000]\n",
    "\n",
    "# evaluate loaded model on single song\n",
    "test_genre = \"blues\"\n",
    "#own_audio = extract_song_features(\"../recordings/output.wav\")\n",
    "some_audio = extract_song_features(\"genres/\" + test_genre + \"/\" + test_genre + \".00000.wav\")\n",
    "\n",
    "# reshape for input\n",
    "test_in = own_audio[np.newaxis,:]\n",
    "test_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c036205c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "775932c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']\n",
      "[1.0825003e-02 9.0991771e-01 3.7754679e-03 1.9286338e-02 5.9297367e-04\n",
      " 1.8619577e-03 1.6781920e-05 3.7434977e-03 4.1233424e-02 8.7468550e-03]\n"
     ]
    }
   ],
   "source": [
    "# now predict on test in\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "score = model.predict(test_in, verbose=0)\n",
    "\n",
    "#np.set_printoptions(precision=2)\n",
    "print(GENRES)\n",
    "print(score[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e270545b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real genre: blues\n",
      "predicted genre: classical\n"
     ]
    }
   ],
   "source": [
    "# now output the name\n",
    "max_class = score.argmax(axis=-1)\n",
    "print(f\"real genre: {test_genre}\\npredicted genre: {GENRES[max_class[0]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ef5823",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "812fc9b9",
   "metadata": {},
   "source": [
    "# Classifying genre with support vector machine\n",
    "\n",
    "Replace the straightforward neural network with a SVM with RBF kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "405439ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 25000)\n",
      "(300, 3)\n"
     ]
    }
   ],
   "source": [
    "# store features and labels\n",
    "features = np.stack(all_features)\n",
    "labels = onehot_labels\n",
    "\n",
    "# last column has genre, turn it into unique ids\n",
    "all_data = np.column_stack((features, labels))\n",
    "\n",
    "# split into training and test data\n",
    "train_input, train_labels, test_input, test_labels = split_train_test(all_data, n_classes=3)\n",
    "svm_labels = np.array([np.where(r==1)[0][0] for r in train_labels])\n",
    "\n",
    "print(np.shape(features))\n",
    "print(np.shape(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6b65e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 3)\n"
     ]
    }
   ],
   "source": [
    "# create the svm model\n",
    "def build_svm():\n",
    "    clf = svm.SVC(kernel='rbf',C=1.0)\n",
    "    return clf\n",
    "\n",
    "print(np.shape(train_labels))\n",
    "\n",
    "# train the svm\n",
    "classifier = build_svm()\n",
    "classifier.fit(train_input, svm_labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "882eab2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'classical'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict datasetTest with svm\n",
    "test_genre = \"jazz\"\n",
    "\n",
    "# either use studio or recorded audio\n",
    "audio = extract_song_features(\"../recordings/output.wav\")\n",
    "#audio = extract_song_features(\"genres/\" + test_genre + \"/\" + test_genre + \".00000.wav\")\n",
    "\n",
    "test_in = audio[np.newaxis,:]\n",
    "predictY = classifier.predict(test_in)\n",
    "GENRES[predictY[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b718b3",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec8ef0d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((300, 25000), (300,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pre processing\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(features)\n",
    "X = scaler.transform(features)\n",
    "y = np.array([np.where(r==1)[0][0] for r in labels])\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a18329b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: \t0.9333333333333333\n",
      "Accuracy: \t0.9333333333333333\n",
      "Accuracy: \t0.9\n",
      "Accuracy: \t0.8666666666666667\n",
      "Accuracy: \t0.8666666666666667\n",
      "Accuracy: \t0.9\n",
      "Accuracy: \t0.9333333333333333\n",
      "Accuracy: \t0.9\n",
      "Accuracy: \t0.9666666666666667\n",
      "Accuracy: \t0.9666666666666667\n",
      "\n",
      "accuracy mean: 0.9166666666666666\n",
      "classical 0.905\n",
      "jazz 0.871\n",
      "metal 0.979\n"
     ]
    }
   ],
   "source": [
    "score = []\n",
    "rates = []\n",
    "# KFold\n",
    "kf = KFold(n_splits=10, shuffle=True)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    # Split data to train and test set\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Train\n",
    "    clf = build_svm()\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Print accuracy\n",
    "    score.append(clf.score(X_test, y_test))\n",
    "    print(\"Accuracy: \\t\" + str(clf.score(X_test, y_test)))\n",
    "\n",
    "    # I make the predictions\n",
    "    predicted = clf.predict(X_test)\n",
    "\n",
    "    # I obtain the confusion matrix\n",
    "    cm = confusion_matrix(y_test, predicted)\n",
    "\n",
    "    # rate calculation\n",
    "    tp_rate = []\n",
    "    i = 0\n",
    "    for row in cm:\n",
    "        current = 0\n",
    "        TP = 0\n",
    "        FP = 0\n",
    "        for g in row:\n",
    "            if current == i:\n",
    "                TP = g\n",
    "            else:\n",
    "                FP = FP + g\n",
    "            current = current + 1\n",
    "        tp_rate.append(TP / (TP + FP))\n",
    "        i = i + 1\n",
    "    rates.append(tp_rate)\n",
    "\n",
    "rates = np.round(np.mean(rates, axis=0), 3)\n",
    "print(\"\")\n",
    "print(\"accuracy mean:\", np.mean(score))\n",
    "i = 0\n",
    "for r in rates:\n",
    "    print(GENRES[i], r)\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c53d1be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f5d72d4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c54f24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
