{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "287418a1",
   "metadata": {},
   "source": [
    "# Genre Identifier\n",
    "\n",
    "Creates a neural network that recognizes the genre of a song\n",
    "\n",
    "feature explanation courtesy of:\n",
    "https://navdeepsinghh.medium.com/identifying-the-genre-of-a-song-with-neural-networks-851db89c42f0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "baf5224f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.feature\n",
    "import librosa.display\n",
    "import glob\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "from scipy.signal import butter, lfilter\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout,  Conv1D, Conv2D, Flatten, BatchNormalization, ZeroPadding2D,  MaxPooling2D, GlobalMaxPooling2D, GlobalAveragePooling1D, AveragePooling2D, Input, Add\n",
    "from keras.utils.np_utils import to_categorical\n",
    "# from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc09bce6",
   "metadata": {},
   "source": [
    "## 1. Load the data\n",
    "\n",
    "Load the data into one vector containg all the information. Data we use for training and testing is the GTZAN data set (https://www.tensorflow.org/datasets/catalog/gtzan)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c65822c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_f(file):\n",
    "    lo, hi = 600,10000\n",
    "    y, sr = librosa.load(file)\n",
    "    b, a = butter(N=6, Wn=[2*lo/sr, 2*hi/sr], btype='band')\n",
    "    x = lfilter(b,a,y)\n",
    "    return x, sr\n",
    "\n",
    "def extract_song_features(f):\n",
    "    y, _ = librosa.load(f)\n",
    "    #y, sr = filter_f(f)\n",
    "\n",
    "    # get Mel-frequency cepstral coefficients and normalize\n",
    "    mfcc = librosa.feature.mfcc(y)\n",
    "    mfcc /= np.amax(np.absolute(mfcc))\n",
    "    return np.ndarray.flatten(mfcc)[:25000]\n",
    "    \n",
    "    # get melspectrogram\n",
    "    #spect = librosa.feature.melspectrogram(y=y, sr=sr,n_fft=2048, hop_length=512)\n",
    "    #spect = librosa.power_to_db(spect, ref=np.max)\n",
    "    #spect /= np.amax(np.absolute(spect))\n",
    "    #return np.ndarray.flatten(spect)[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7d9f42c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 100 songs in blues genre...\n",
      "Processing 100 songs in classical genre...\n",
      "Processing 100 songs in country genre...\n",
      "Processing 100 songs in disco genre...\n",
      "Processing 100 songs in hiphop genre...\n",
      "Processing 100 songs in jazz genre...\n",
      "Processing 100 songs in metal genre...\n",
      "Processing 100 songs in pop genre...\n",
      "Processing 100 songs in reggae genre...\n",
      "Processing 100 songs in rock genre...\n"
     ]
    }
   ],
   "source": [
    "GENRES = ['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']\n",
    "all_features = []\n",
    "all_labels = []\n",
    "\n",
    "# load all songs from the gtzan data set\n",
    "for genre in GENRES:\n",
    "    sound_files = glob.glob('genres/' + genre + '/*.wav')\n",
    "    print('Processing %d songs in %s genre...' % (len(sound_files), genre))\n",
    "    for f in sound_files:\n",
    "        extracted_features = extract_song_features(f)\n",
    "        all_features.append(extracted_features)\n",
    "        all_labels.append(genre)\n",
    "\n",
    "# convert labels to one-hot encoding\n",
    "label_uniq_ids, label_row_ids = np.unique(all_labels, return_inverse=True)\n",
    "label_row_ids = label_row_ids.astype(np.int32, copy=False)\n",
    "onehot_labels = to_categorical(label_row_ids, len(label_uniq_ids))\n",
    "\n",
    "# store features and labels\n",
    "features = np.stack(all_features)\n",
    "labels = onehot_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0d7309a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 25000)\n",
      "(1000, 10)\n"
     ]
    }
   ],
   "source": [
    "# get shapes\n",
    "print(np.shape(features))\n",
    "print(np.shape(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56bf6ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "208f478e",
   "metadata": {},
   "source": [
    "## 2. Prepare the data set and build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "53856691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(data, training_split=0.8):\n",
    "    \"\"\"\n",
    "    this splits according to the ration we want to split with\n",
    "    \"\"\"\n",
    "    np.random.shuffle(data)\n",
    "    split_idx = int(len(data) * training_split)\n",
    "    train, test = data[:split_idx, :], data[split_idx:, :]\n",
    "\n",
    "    #      train data,     train labels,  test data,      test labels\n",
    "    return train[:, :-10], train[:, -10:], test[:, :-10], test[:, -10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0b890b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(train):\n",
    "    \"\"\"\n",
    "    build the model; in our case it is a perceptron with 100 layers\n",
    "    \"\"\"\n",
    "    input_shape = np.shape(train[0])\n",
    "    print(f\"shape of input data: {input_shape}\")\n",
    "    #nn_model = Sequential([\n",
    "    #    Dense(100, input_dim=np.shape(train)[1]),\n",
    "    #    Activation('relu'),\n",
    "    #    Dense(10),\n",
    "    #    Activation('softmax'),\n",
    "    #])\n",
    "    \n",
    "    nn_model = Sequential()\n",
    "    nn_model.add(Conv2D(100, 3, input_shape=input_shape))\n",
    "    nn_model.add(AveragePooling2D((2, 2), strides=(2,2)))\n",
    "    nn_model.add(Activation('relu'))\n",
    "    nn_model.add(Flatten())\n",
    "    nn_model.add(Dropout(rate=0.5))\n",
    "    nn_model.add(Dense(100,input_shape=input_shape))\n",
    "    nn_model.add(Activation('relu'))\n",
    "    nn_model.add(Dropout(rate=0.5))\n",
    "    nn_model.add(Dense(10))\n",
    "    nn_model.add(Activation('softmax'))\n",
    "\n",
    "    # specify which techniques you want to use for training\n",
    "    nn_model.compile(optimizer='adam',\n",
    "                     loss='categorical_crossentropy',\n",
    "                     metrics=['accuracy'])\n",
    "    print(nn_model.summary())\n",
    "    return nn_model\n",
    "\n",
    "# alternatively use svm\n",
    "def build_svm():\n",
    "    clf = svm.SVC(kernel='rbf',C=1.0)\n",
    "    return clf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b1735c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of input data: (125, 200, 1)\n",
      "WARNING:tensorflow:From /home/cornelius/anaconda3/envs/robocup/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4074: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 123, 198, 100)     1000      \n",
      "_________________________________________________________________\n",
      "average_pooling2d_1 (Average (None, 61, 99, 100)       0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 61, 99, 100)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 603900)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 603900)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               60390100  \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1010      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 60,392,110\n",
      "Trainable params: 60,392,110\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "shape of input data: (1, 800, 25000)\n"
     ]
    }
   ],
   "source": [
    "# last column has genre, turn it into unique ids\n",
    "all_data = np.column_stack((features, labels))\n",
    "\n",
    "# split into training and test data\n",
    "train_input, train_labels, test_input, test_labels = split_train_test(all_data)\n",
    "\n",
    "# Reshape for CNN input\n",
    "train_in = np.array([x.reshape( (125, 200, 1) ) for x in train_input])\n",
    "test_in = np.array([x.reshape( (125, 200, 1) ) for x in test_input])\n",
    "\n",
    "# build the nn model\n",
    "model = build_model(train_in)\n",
    "print(f\"shape of input data: {np.shape(train_input[np.newaxis,:,:])}\")\n",
    "\n",
    "# train the svm\n",
    "#svm_labels = [np.where(r==1)[0][0] for r in train_labels]\n",
    "#classifier = build_svm()\n",
    "#classifier.fit(train_input, svm_labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ff16acac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 25010)\n",
      "(800, 25000)\n",
      "(800, 10)\n"
     ]
    }
   ],
   "source": [
    "print(all_data.shape)\n",
    "print(train_input.shape)\n",
    "print(train_labels.shape)\n",
    "#print(train_in.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5105c82b",
   "metadata": {},
   "source": [
    "## 3. Run the process\n",
    "\n",
    "Now, we train our model and evaluate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96f56e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8ab98e84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'reggae'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict datasetTest with svm\n",
    "#test_genre = \"disco\"\n",
    "#own_audio = extract_song_features(\"../recordings/output.wav\")\n",
    "#some_audio = extract_song_features(\"genres/\" + test_genre + \"/\" + test_genre + \".00000.wav\")\n",
    "#test_in = some_audio[np.newaxis,:]\n",
    "#predictY = classifier.predict(test_in)\n",
    "#GENRES[predictY[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5c40b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/cornelius/anaconda3/envs/robocup/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 640 samples, validate on 160 samples\n",
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "model.fit(train_in, train_labels, epochs=10, batch_size=32,\n",
    "          validation_split=0.2)\n",
    "\n",
    "# now get the performance indicators\n",
    "loss, acc = model.evaluate(test_in, test_labels, batch_size=32)\n",
    "\n",
    "print(\"Done!\")\n",
    "print(\"Loss: %.4f, accuracy: %.4f\" % (loss, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13f5caf",
   "metadata": {},
   "source": [
    "## 4. Store the classifier to use it in thinking.py\n",
    "\n",
    "use JSON for simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "76d95f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "model.save(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49df6159",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36daa1b2",
   "metadata": {},
   "source": [
    "## 5. Retrieve model and test it to see if everything works\n",
    "\n",
    "make sure you have the correct version of h5py (2.10.0). This creates warnings at one end, but was the only thing that worked for me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "01208bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 100)               2500100   \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                1010      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 2,501,110\n",
      "Trainable params: 2,501,110\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model = load_model('model.h5')\n",
    "print(\"Loaded model from disk\")\n",
    "# summarize model.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6bd409cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 25000)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y, _ = librosa.load(\"../recordings/output.wav\")\n",
    "# get Mel-frequency cepstral coefficients and normalize\n",
    "mfcc = librosa.feature.mfcc(y)\n",
    "mfcc /= np.amax(np.absolute(mfcc))\n",
    "own_audio = np.ndarray.flatten(mfcc)[:25000]\n",
    "\n",
    "# evaluate loaded model on single song\n",
    "test_genre = \"blues\"\n",
    "#own_audio = extract_song_features(\"../recordings/output.wav\")\n",
    "some_audio = extract_song_features(\"genres/\" + test_genre + \"/\" + test_genre + \".00000.wav\")\n",
    "\n",
    "# reshape for input\n",
    "test_in = own_audio[np.newaxis,:]\n",
    "test_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c036205c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "775932c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']\n",
      "[1.0825003e-02 9.0991771e-01 3.7754679e-03 1.9286338e-02 5.9297367e-04\n",
      " 1.8619577e-03 1.6781920e-05 3.7434977e-03 4.1233424e-02 8.7468550e-03]\n"
     ]
    }
   ],
   "source": [
    "# now predict on test in\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "score = model.predict(test_in, verbose=0)\n",
    "\n",
    "#np.set_printoptions(precision=2)\n",
    "print(GENRES)\n",
    "print(score[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e270545b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real genre: blues\n",
      "predicted genre: classical\n"
     ]
    }
   ],
   "source": [
    "# now output the name\n",
    "max_class = score.argmax(axis=-1)\n",
    "print(f\"real genre: {test_genre}\\npredicted genre: {GENRES[max_class[0]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ef5823",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
